# -*- coding: utf-8 -*-
"""rag-Starter-Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aDwD8QvUWhpmq_xYYHGjYEI5F3CpWmt_
"""

# Load all required Libraries
import pandas as pd
import numpy as np
import transformers, torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset

from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

COLLECTION_NAME = "rag_mini" #config["COLLECTION_NAME"]
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
VECTOR_DIMENSION = 384
GENERATION_MODEL_NAME = "google/flan-t5-base"
EMBEDDING_FIELD = "embedding"

"""# Read Passages from the Datasets and Drop rows if they are NA or empty"""

passages = pd.read_parquet("hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet")

print(passages.shape)
passages.head()

"""# Do EDA on the passage dataset
- You can try to find the maximum and minimum length of the passages before indexing (just a direction)
"""

# Code for EDA
import numpy as np
#length in terms of characters
passages['length_char'] = passages['passage'].apply(len)

print(f"Total number of passages: {len(passages)}")
print(f"Min passage length (chars): {passages['length_char'].min()}")
print(f"Max passage length (chars): {passages['length_char'].max()}")
print(f"Avg passage length (chars): {passages['length_char'].mean():.2f}")
print(f"SD of length (chars): {passages['length_char'].std():.2f}")


# length in terms of tokens-word count
passages['word_count'] = passages['passage'].apply(lambda x: len(str(x).split()))

print(f"Min word count: {passages['word_count'].min()}")
print(f"Max word count: {passages['word_count'].max()}")
print(f"Avg word count: {passages['word_count'].mean():.2f}")

# Remove the temporary columns added for analysis
passages = passages.drop(columns=['length_char', 'word_count'])

# Display the head again to confirm the DataFrame is clean
print("\nPassages DataFrame head:")
print(passages.head())

"""# Tokenize Text and Generate Embeddings using Sentence Transformers"""

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

# Encode Text
embeddings = embedding_model.encode(passages['passage'].tolist()).tolist()

print(f"Embeddings row: ({len(embeddings)})")
print(f"Embeddings column: ({len(embeddings[0])})")

# Update passages DataFrame with embeddings
passages['embedding'] = embeddings

"""# Create Milvus Client and Insert your Embeddings to your DB
- Make sure you define a schema for your collection (Points will be deducted if you fail to define a proper schema with ids, passage text, embedding)
"""

# Define every column of your schema
id_ = FieldSchema(
    name="id",
    dtype=DataType.INT64,
    is_primary=True,
    auto_id=False,
    description="Passage ID"
)
#max_length can adjust, put large number just in case
passage = FieldSchema(
    name="passage",
    dtype=DataType.VARCHAR,
    max_length=10000,
    description="Raw passage text"
)
# vector field. all-MiniLM-L6-v2 is 384 dimensions
embedding = FieldSchema(
    name="embedding",
    dtype=DataType.FLOAT_VECTOR,
    dim=VECTOR_DIMENSION,
    description="Vector embedding"
)

# define a proper schema with ids, passage text, embedding mentioned above
schema = CollectionSchema(
    fields=[id_, passage, embedding],
    description="RAG-Mini-Wikipedia Collection"
)

client = MilvusClient("rag_wikipedia_mini.db")
client
# Create the Collection with Collection Name = "rag_mini". Make sure you define the schema variable while creating the collection

# print(client.list_collections())

# Remove the collection if it already exists for a fresh run

if client.has_collection(collection_name=COLLECTION_NAME):
    client.drop_collection(collection_name=COLLECTION_NAME)
    print(f"Dropped existing collection: {COLLECTION_NAME}")

# add error handling below as well
try:
  client.create_collection(
      collection_name=COLLECTION_NAME,
      schema=schema
  )
  print(f"Created new collection: {COLLECTION_NAME}")
except Exception as e:
    print(f"Error creating collection: {e}")

"""**Convert your Pandas Dataframe to a list of dictionaries**
- The Dictionary at least have 3 keys [id, passage, embedding]
"""

passages_reset = passages.reset_index()

rag_data = rag_data = passages_reset[['id', 'passage', 'embedding']].to_dict('records')

print(f"First data point for insertion: {rag_data[0]}")

# Code to insert the data to your DB
res = client.insert(collection_name=COLLECTION_NAME, data=rag_data)
print(res)

"""- Do a Sanity Check on your database

**Do not delete the below line during your submission**
"""

print("Entity count:", client.get_collection_stats("rag_mini")["row_count"])
print("Collection schema:", client.describe_collection("rag_mini"))

"""# Steps to Fetch Results
- Read the Question Dataset
- Clean the Question Dataset if necessary (Drop Questions with NaN etc.)
- Convert Each Query to a Vector Embedding (Use the same embedding model you used to embed your document)
- Try for a Single Question First
- Load Collection into Memory after creating Index for Search on your embedding field (This is an essential step before you can search in your db)
- Search and Fetch Top N Results
"""

import pandas as pd

queries = pd.read_parquet("hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet")
queries

#drop na in the dataset
queries_df = queries.dropna(subset=['question', 'answer'])
query = queries_df['question'].iloc[0]                  # Your single query

query_embedding = embedding_model.encode(query).tolist()

# print(query_embedding.shape)

print(f"Query: {query}")
print(f"Shape of query embedding: {np.array(query_embedding).shape}")

"""#### Create Index on the embedding column on your DB"""

index_params = MilvusClient.prepare_index_params()

# Add an index on the embedding field
index_params.add_index(
    field_name=EMBEDDING_FIELD,
    index_name="vector_index",
    index_type="AUTOINDEX",  # Use AUTOINDEX for simplicity and Milvus Lite compatibility
    metric_type="COSINE"     # Use COSINE similarity, common for sentence embeddings
)

# Create the index
try:
    client.create_index(collection_name=COLLECTION_NAME, index_params=index_params)
    print("Index created successfully.")
except Exception as e:
    print(f"Index creation result error: {e}")

# Load collection into memory (required for search)
client.load_collection(collection_name=COLLECTION_NAME)
print("Collection loaded into memory")

# Search the db with your query embedding
search_params = {
    "data": [query_embedding],
    "collection_name": COLLECTION_NAME,
    "limit": 1,  # Fetch Top result
    "output_fields": ["passage"], # We only need the passage text
}

# Search the db with your query embedding

output_ = client.search(**search_params)

print(output_)

"""## Now get the Context
- Initially use the first passage ONLY as your context
- In Later Experiments, you must try at least 2 different passage selection strategies (Top 3 / Top 5 / Top 10) and pass to your prompt
"""

context = "No relevant context found."

# observed output structure: data: [[{..., 'entity': {'passage': '...'}}]]

# 1. Check if results were returned at all
if output_ and len(output_) > 0:

    # Access the result list for the first query (usually output_[0])
    first_query_result_list = output_[0]

    # 2. Check if the inner list contains any hits
    if isinstance(first_query_result_list, list) and len(first_query_result_list) > 0:

        # Get the Top-1 hit (the first dictionary in the inner list)
        top_1_hit = first_query_result_list[0]

        # 3. Extract the passage nested under the 'entity' key
        # This structure is specific to Milvus Lite in some environments.
        if 'entity' in top_1_hit and 'passage' in top_1_hit['entity']:
            context = top_1_hit['entity']['passage']

            print("Selected Context (Top-1):")
            print(context)
        else:
            # Should not happen if data was found, but handles structure change
            print("Error: Context found, but 'passage' key missing in 'entity'.")
    else:
        # Context list was empty (no documents were close enough)
        print("Search returned an empty hits list.")
else:
    # Output_ was empty or not a list (indicates retrieval failure)
    print("No search results returned at all (Retrieval may have failed).")

"""**Develop your Prompt**"""

# prompt test
system_prompt = (
    "You are an intelligent, helpful and highy accurate QA assistant. "
    "Please based on and only based on my provided context to answer questions. "
    "If any answer is not present in the context, say you didn't find the answer. "
    "No external knowledge usage allowed."
)

prompt = f"""{system_prompt} \n\nContext: {context} \n\nQuestion: {query}"""
print(prompt)

"""# RAG Response for a Single Query"""

# Load the LLM Model you want to use
MODEL_NAME  = GENERATION_MODEL_NAME

# Use AutoModelForSeq2SeqLM for encoder-decoder models like T5
llm_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Prepare input for the model
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# Generate answer
output_tokens = llm_model.generate(
    input_ids,
    max_length=50,
    do_sample=False,
    temperature=0.0
)

# Decode and extract answer.
rag_answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

print("RAG Generated Answer Shown Below")
print(f"Question: {query}")
print(f"Answer: {rag_answer}")

"""# Generate Responses for all the Queries in the Dataset"""

def context_retrieve(question, client, k=1):
    """Retrieves context"""

    # Retrieval + Error Handling
    try:
        query_embedding = embedding_model.encode(question).tolist()
        search_params = {
            "data": [query_embedding],
            "collection_name": COLLECTION_NAME,
            "limit": k,
            "output_fields": ["passage"],
        }
        search_results = client.search(**search_params)
    except Exception as e:
        print(f"Retrieval Error: {e}")
        return "Retrieval failed.", []

    # Context Extraction + Error Handling
    contexts = []

    # 1. Check if results were returned at all
    if search_results and len(search_results) > 0:

        # Access the result list for the first query
        first_query_result_list = search_results[0]

        # 2. Check if the inner list contains any hits
        if isinstance(first_query_result_list, list) and len(first_query_result_list) > 0:

            # Loop over all hits in the inner list
            for hit in first_query_result_list:
                if 'passage' in hit:
                    contexts.append(hit['passage'])
                elif 'entity' in hit and 'passage' in hit['entity']:
                    # Fallback for nested structure
                    contexts.append(hit['entity']['passage'])

            if contexts:
                context_str = "\n".join(contexts)
            else:
                context_str = "No relevant context found in hits."
        else:
            context_str = "Search returned an empty hits list."
    else:
        context_str = "No search results returned at all (Retrieval may have failed)."

    print("All Collected Contexts Shown Below")
    print(context_str)

    return contexts, context_str

#instruct prompt
def RAG_generator(question, client, llm_model, tokenizer, k=1):
    """constructs prompt, and generates RAG response."""

    contexts, context_str = context_retrieve(question, client, k)

    # Prompt Construction
    system_prompt = (
        "You are an intelligent, helpful and highy accurate QA assistant. "
        "Please based on and only based on my provided context to answer questions. "
        "If any answer is not present in the context, say you didn't find the answer. "
        "No external knowledge usage allowed."
    )

    prompt = f"""{system_prompt} \n\nContext: {context_str} \n\nQuestion: {question}"""

    # Generation + Error Handling
    try:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        output_tokens = llm_model.generate(input_ids, max_length=50, do_sample=False, temperature=0.0)
        rag_answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
    except Exception as e:
        rag_answer = f"Generation failed: {e}"

    return rag_answer, contexts


# Code to run the loop remains the same
all_questions = queries_df['question'].tolist()
all_ground_truths = queries_df['answer'].tolist()
all_generated_answers = []
all_retrieved_contexts = []
sample_size = min(100, len(queries_df))

for i in range(sample_size):
    question = all_questions[i]
    answer, contexts = RAG_generator(question, client, llm_model, tokenizer, k=1)
    all_generated_answers.append(answer)
    all_retrieved_contexts.append(contexts)

all_ground_truths_sampled = all_ground_truths[:sample_size]
all_questions_sampled = all_questions[:sample_size]

#persona prompt
def RAG_persona_generator(question, client, llm_model, tokenizer, k=1):
    """constructs prompt, and generates RAG response."""

    contexts, context_str = context_retrieve(question, client, k)

    # Prompt Construction
    system_prompt = (
        "You are a distinguished history professor in a top-tier university. You must answer students' RAG relevant questions accurately and concisely, and you should strictly based on the provided historical information."
        "If the context is not enough for you to get the final answer, say 'More historical information are needed'."

    )

    prompt = f"""{system_prompt} \n\nContext: {context_str} \n\nQuestion: {question}"""

    # Generation + Error Handling
    try:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        output_tokens = llm_model.generate(input_ids, max_length=50, do_sample=False, temperature=0.0)
        rag_answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
    except Exception as e:
        rag_answer = f"Generation failed: {e}"

    return rag_answer, contexts

# Code to run the loop remains the same
all_questions_persona = queries_df['question'].tolist()
all_ground_truths_persona = queries_df['answer'].tolist()
all_generated_answers_persona = []
all_retrieved_contexts_persona = []

for i in range(sample_size):
    question_persona = all_questions_persona[i]
    answer_persona, contexts_persona = RAG_persona_generator(question_persona, client, llm_model, tokenizer, k=1)
    all_generated_answers_persona.append(answer_persona)
    all_retrieved_contexts_persona.append(contexts_persona)

all_ground_truths_sampled_persona = all_ground_truths_persona[:sample_size]
all_questions_sampled_persona = all_questions_persona[:sample_size]

"""# Finding out the Basic QA Metrics (F1 score, EM score)"""

from evaluate import load

# Load the SQuAD evaluation metric (Requires 'evaluate' library)
squad_metric = load("squad")

# Prepare data for SQuAD metric
question_ids = [str(i) for i in range(sample_size)]
predictions = [{'prediction_text': pred, 'id': q_id} for q_id, pred in zip(question_ids, all_generated_answers)]
references = [{'answers': {'answer_start': [0], 'text': [gt]}, 'id': q_id} for q_id, gt in zip(question_ids, all_ground_truths_sampled)]

# Calculate the metric
results = squad_metric.compute(predictions=predictions, references=references)

print("Instruction Prompt Metrics (SQuAD EM/F1)")
print(f"Exact Match (EM): {results['exact_match']:.2f}%")
print(f"F1 Score: {results['f1']:.2f}%")

# Prepare data for SQuAD metric
question_ids = [str(i) for i in range(sample_size)]
predictions_persona = [{'prediction_text': pred, 'id': q_id} for q_id, pred in zip(question_ids, all_generated_answers_persona)]
references_persona = [{'answers': {'answer_start': [0], 'text': [gt]}, 'id': q_id} for q_id, gt in zip(question_ids, all_ground_truths_sampled_persona)]

# Calculate the metric
results_persona = squad_metric.compute(predictions=predictions_persona, references=references_persona)

print("Persona Prompt Metrics (SQuAD EM/F1)")
print(f"Exact Match (EM): {results_persona['exact_match']:.2f}%")
print(f"F1 Score: {results_persona['f1']:.2f}%")

# add for step 4
def calculate_squad_metrics(generated_answers, ground_truths):
    squad_metric = load("squad")
    question_ids = [str(i) for i in range(len(generated_answers))]
    predictions = [{'prediction_text': pred, 'id': q_id} for q_id, pred in zip(question_ids, generated_answers)]
    references = [{'answers': {'answer_start': [0], 'text': [gt]}, 'id': q_id} for q_id, gt in zip(question_ids, ground_truths)]
    return squad_metric.compute(predictions=predictions, references=references)

#step 4
def RAG_generator_exp(question, client, llm_model, tokenizer, k, emb_model, coll_name, prompt_strategy="instruction"):
    """
    Retrieves context, constructs prompt, and generates RAG response for experiments.
    Accepts embedding model and collection name as arguments.
    """

    # 1. Retrieval
    try:
        query_embedding = emb_model.encode(question).tolist()
        search_params = {
            "data": [query_embedding],
            "collection_name": coll_name,
            "limit": k,
            "output_fields": ["passage"],
        }
        search_results = client.search(**search_params)
    except Exception as e:
        return "Retrieval failed.", []

    # 2. Context Extraction
    contexts = []
    if search_results and len(search_results) > 0:
        first_query_result_list = search_results[0]
        if isinstance(first_query_result_list, list) and len(first_query_result_list) > 0:
            for hit in first_query_result_list:
                if 'entity' in hit and 'passage' in hit['entity']:
                    contexts.append(hit['entity']['passage'])

    context_str = "\n".join(contexts)

    # 3. Prompt Construction
    system_prompt = (
        "You are an intelligent, helpful and highy accurate QA assistant. "
        "Please based on and only based on my provided context to answer questions. "
        "If any answer is not present in the context, say you didn't find the answer. "
        "No external knowledge usage allowed."
    )
    prompt = f"""{system_prompt} \n\nContext: {context_str} \n\nQuestion: {question}"""

    # 4. Generation
    try:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        output_tokens = llm_model.generate(input_ids, max_length=50, do_sample=False, temperature=0.0)
        rag_answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
    except Exception as e:
        rag_answer = f"Generation failed: {e}"

    return rag_answer, contexts

#step 4
def run_parameter_experiment(config_name, config_details, queries_df, passages_df):
    """Orchestrates the indexing, generation, and evaluation for one configuration."""

    model_name = config_details['model_name']
    dimension = config_details['dimension']
    k_values = config_details['k_values']

    exp_collection_name = f"rag_exp_{dimension}"

    # Re-indexing
    print(f"Starting Indexing for {config_name} (Dim: {dimension})")
    exp_embedding_model = SentenceTransformer(model_name)

    # create new collections
    if client.has_collection(exp_collection_name):
        client.drop_collection(exp_collection_name)

    schema = CollectionSchema(
        fields=[
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=False),
            FieldSchema(name="passage", dtype=DataType.VARCHAR, max_length=8192),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
        ]
    )
    client.create_collection(collection_name=exp_collection_name, schema=schema)

    # embedding and inserting
    exp_embeddings = exp_embedding_model.encode(passages_df['passage'].tolist()).tolist()
    passages_exp = passages_df.reset_index().copy()
    passages_exp['embedding'] = exp_embeddings
    rag_data_exp = passages_exp[['id', 'passage', 'embedding']].to_dict('records')
    client.insert(collection_name=exp_collection_name, data=rag_data_exp)

    # create index
    index_params = MilvusClient.prepare_index_params()
    index_params.add_index(field_name="embedding", index_type="AUTOINDEX", metric_type="COSINE")
    client.create_index(collection_name=exp_collection_name, index_params=index_params)
    client.load_collection(collection_name=exp_collection_name)
    print("Indexing Complete.")

    # Evaluation Loop
    all_experiment_results = []

    for k in k_values:
        print(f" Testing {config_name} with k={k}...")

        all_generated_answers_exp = []

        for question in queries_df['question'].tolist()[:sample_size]:
            answer, _ = RAG_generator_exp(
                question, client, llm_model, tokenizer, k,
                emb_model=exp_embedding_model,
                coll_name=exp_collection_name
            )
            all_generated_answers_exp.append(answer)

        # F1/EM Metrics
        results = calculate_squad_metrics(all_generated_answers_exp, all_ground_truths_sampled)

        all_experiment_results.append({
            "Config": config_name,
            "Dimension": dimension,
            "Retrieval_k": k,
            "EM": results['exact_match'],
            "F1": results['f1'],
        })

    client.release_collection(exp_collection_name)
    return all_experiment_results

EXPERIMENT_CONFIG = {
    "384_dim": {
        "model_name": "all-MiniLM-L6-v2",
        "dimension": 384,
        "k_values": [3, 5]
    },
    "512_dim": {
        "model_name": "sentence-transformers/distiluse-base-multilingual-cased-v2",
        "dimension": 512,
        "k_values": [3, 5]
    }
}

final_experiment_results = []
for name, config in EXPERIMENT_CONFIG.items():

    results = run_parameter_experiment(name, config, queries_df, passages)
    final_experiment_results.extend(results)

final_df = pd.DataFrame(final_experiment_results)
print(final_df)
# final_df.to_csv("comparison_analysis.csv", index=False)

"""Task 5"""

# Cell for Reranking Model Setup
from sentence_transformers import CrossEncoder
import torch.nn.functional as F

# Initialize Cross-Encoder Reranker
RERANKER_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
reranker = CrossEncoder(RERANKER_MODEL_NAME)

print(f"Reranker Model Loaded: {RERANKER_MODEL_NAME}. Proceed with Enhanced RAG function.")

# ENHANCEMENT 1: Query Rewriting
def rewrite_query(question, llm_model, tokenizer):
    """Rewrites an ambiguous question into a better search query using the LLM."""
    rewrite_prompt = (
        f"Rewrite the following question to be an effective search query. Original Question: '{question}'"
    )

    input_ids = tokenizer(rewrite_prompt, return_tensors="pt").input_ids
    output_tokens = llm_model.generate(input_ids, max_length=1000)
    rewritten_query = tokenizer.decode(output_tokens[0], skip_special_tokens=True)

    # Simple check to avoid empty or garbage output
    if len(rewritten_query) < 5 or rewritten_query.startswith("Rewrite"):
        return question # Return original question if rewriting fails
    return rewritten_query

# ENHANCEMENT 2: Reranking
def rerank_documents(query, contexts, top_k=1):
    """Reranks retrieved documents using a Cross-Encoder and selects the top k."""

    # Prepare (query, context_i) pairs for cross-encoder
    model_inputs = [[query, context] for context in contexts]

    # Get scores from the Cross-Encoder
    scores = reranker.predict(model_inputs) # output is a logit score representing relevance

    # Pair scores with contexts and sort
    ranked_results = sorted(zip(scores, contexts), key=lambda x: x[0], reverse=True)

    # Return the top k doc
    return [context for score, context in ranked_results[:top_k]]


def RAG_enhanced_generator(question, client, llm_model, tokenizer, k_retrieve=5, k_final=1):
    """ Enhanced RAG function with above Query Rewriting and Reranking """
    # Query Rewriting (Enhancement 1)
    search_query = rewrite_query(question, llm_model, tokenizer)

    # Initial Retrieval (similar to previous coding cell)
    try:
        query_embedding = embedding_model.encode(search_query).tolist()
        search_params = {
            "data": [query_embedding],
            "collection_name": COLLECTION_NAME,
            "limit": k_retrieve,
            "output_fields": ["passage"],
        }
        search_results = client.search(**search_params)
    except Exception as e:
        return "Retrieval failed.", []

    initial_contexts = []

    # checking a before
    if search_results and len(search_results) > 0:
        first_query_result = search_results[0]

        hits_list = None
        if isinstance(first_query_result, list):
            hits_list = first_query_result
        elif isinstance(first_query_result, dict) and 'hits' in first_query_result:
            # Fallback
            hits_list = first_query_result['hits']

        if hits_list:
            for hit in hits_list:
                # extract passage text
                if 'entity' in hit and 'passage' in hit['entity']:
                    initial_contexts.append(hit['entity']['passage'])
                elif 'passage' in hit:
                    initial_contexts.append(hit['passage'])

    if not initial_contexts:
        return "No relevant context retrieved.", []

    # Reranking (Enhancement 2)
    final_contexts = rerank_documents(question, initial_contexts, top_k=k_final)

    context_str = "\n".join(final_contexts)

    # Generation (using Instruction Prompt as verified in step 3)
    system_prompt = ("You are an intelligent, helpful and highy accurate QA assistant. "
                    "Please based on and only based on my provided context to answer questions. "
                    "If any answer is not present in the context, say you didn't find the answer. "
                    "No external knowledge usage allowed.")

    prompt = f"""{system_prompt} \n\nContext: {context_str} \n\nQuestion: {question}"""

    # Generation
    try:
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids
        output_tokens = llm_model.generate(input_ids, max_length=50, do_sample=False)
        rag_answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
    except Exception as e:
        rag_answer = f"Generation failed: {e}"

    return rag_answer, final_contexts

# Initialize Cross-Encoder Reranker
RERANKER_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
reranker = CrossEncoder(RERANKER_MODEL_NAME)

# Re-initialize models/client for execution context stability
client = MilvusClient("rag_wikipedia_mini.db")
COLLECTION_NAME = "rag_mini"
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
MODEL_NAME = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
llm_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

# STEP 5 EVALUATION

# Set the optimal retrieval pool size based on Step 4's findings (k=5)
K_RETRIEVE = 5
K_FINAL = 1 # Rerank down to the single best document for the LLM

all_enhanced_answers = []
all_enhanced_contexts = []
sample_size = min(100, len(queries_df))

print(f"Running Enhanced RAG (Query Rewrite + Rerank) with k_retrieve={K_RETRIEVE}...")

for i in range(sample_size):
    question = queries_df['question'].iloc[i]

    answer, contexts = RAG_enhanced_generator(
        question, client, llm_model, tokenizer,
        k_retrieve=K_RETRIEVE,
        k_final=K_FINAL
    )
    all_enhanced_answers.append(answer)
    all_enhanced_contexts.append(contexts)

# Calculate Metrics (using the same SQuAD function from Step 3/4)
enhanced_results = calculate_squad_metrics(all_enhanced_answers, all_ground_truths_sampled)

print("Enhanced RAG Evaluation ")
print(f"Enhanced EM: {enhanced_results['exact_match']:.2f}%")
print(f"Enhanced F1: {enhanced_results['f1']:.2f}%")

# STEP 5 EVALUATION

# Set the optimal retrieval pool size based on Step 4's findings (k=5)
K_RETRIEVE = 5
K_FINAL = 3 # Rerank down to 3 best document for the LLM

all_enhanced_answers_3 = []
all_enhanced_contexts_3 = []
sample_size = min(100, len(queries_df))

print(f"Running Enhanced RAG (Query Rewrite + Rerank) with k_retrieve={K_RETRIEVE}...")

for i in range(sample_size):
    question = queries_df['question'].iloc[i]

    answer, contexts = RAG_enhanced_generator(
        question, client, llm_model, tokenizer,
        k_retrieve=K_RETRIEVE,
        k_final=K_FINAL
    )
    all_enhanced_answers_3.append(answer)
    all_enhanced_contexts_3.append(contexts)

# Calculate Metrics (using the same SQuAD function from Step 3/4)
enhanced_results_3 = calculate_squad_metrics(all_enhanced_answers_3, all_ground_truths_sampled)

print("\n--- Enhanced RAG Evaluation (F1/EM) ---")
print(f"Enhanced EM: {enhanced_results_3['exact_match']:.2f}%")
print(f"Enhanced F1: {enhanced_results_3['f1']:.2f}%")

# STEP 5 EVALUATION

# Set the optimal retrieval pool size based on Step 4's findings (k=5)
K_RETRIEVE = 5
K_FINAL = 5 # Rerank down to 5 best document for the LLM

all_enhanced_answers_5 = []
all_enhanced_contexts_5 = []
sample_size = min(100, len(queries_df))

print(f"Running Enhanced RAG (Query Rewrite + Rerank) with k_retrieve={K_RETRIEVE}...")

for i in range(sample_size):
    question = queries_df['question'].iloc[i]

    answer, contexts = RAG_enhanced_generator(
        question, client, llm_model, tokenizer,
        k_retrieve=K_RETRIEVE,
        k_final=K_FINAL
    )
    all_enhanced_answers_5.append(answer)
    all_enhanced_contexts_5.append(contexts)

# Calculate Metrics (using the same SQuAD function from Step 3/4)
enhanced_results_5 = calculate_squad_metrics(all_enhanced_answers_5, all_ground_truths_sampled)

print("\n--- Enhanced RAG Evaluation (F1/EM) ---")
print(f"Enhanced EM: {enhanced_results_5['exact_match']:.2f}%")
print(f"Enhanced F1: {enhanced_results_5['f1']:.2f}%")

"""# Advanced Evaluation using RAGAs

follow instructions to run RAGAS
"""

pip install openai ragas langchain-openai

import os
from openai import OpenAI
os.environ["OPENAI_API_KEY"] = "masked api call here"
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

#test one message after using the model and implment ragas test
resp = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role":"user","content":"Hello from RAGAS test"}]
)
print(resp.choices[0].message.content)

#built llm for later llm call
from langchain_openai import ChatOpenAI
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

llm = ChatOpenAI(model="gpt-4o-mini", api_key=os.environ["OPENAI_API_KEY"])

all_questions = queries_df['question'].tolist()
all_ground_truths = queries_df['answer'].tolist()
all_ground_truths_sampled = all_ground_truths[:sample_size]
all_questions_sampled = all_questions[:sample_size]

# the naive baseline evaluation
data_naive = {
    "question": all_questions_sampled,
    "answer": all_generated_answers,
    "contexts": all_retrieved_contexts,
    "reference": all_ground_truths_sampled}

dataset_naive = Dataset.from_dict(data_naive)

# Evaluate naive RAG
results_naive = evaluate(
    dataset=dataset_naive,
    metrics=[faithfulness, answer_relevancy, context_recall, context_precision],
    llm = llm
)

naive_df = results_naive.to_pandas().mean(numeric_only=True)
print("\n Naive RAG (K=5) RAGAs Metrics Calculated Below")
print(naive_df)

# the enhanced RAG evaluation with K_final = 1
data_enhanced = {
    "question": all_questions_sampled,
    "answer": all_enhanced_answers,      # Answers from Enhanced RAG
    "contexts": all_enhanced_contexts,   # Contexts after Reranking (K_FINAL=1)
    "reference": all_ground_truths_sampled
}
dataset_enhanced = Dataset.from_dict(data_enhanced)

# Evaluate Enhanced RAG (K_FINAL=1)
results_enhanced = evaluate(
    dataset=dataset_enhanced,
    metrics=[faithfulness, answer_relevancy, context_recall, context_precision],
    llm = llm
)

print(results_enhanced)

enhanced_df = results_enhanced.to_pandas().mean(numeric_only=True)
print(enhanced_df)

# the enhanced RAG evaluation with K_final = 3

data_enhanced_3 = {
    "question": all_questions_sampled,
    "answer": all_enhanced_answers_3,      # Answers from Enhanced RAG with K_FINAL=3
    "contexts": all_enhanced_contexts_3,   # Contexts after Reranking (K_FINAL=3)
    "reference": all_ground_truths_sampled
}

dataset_enhanced_3 = Dataset.from_dict(data_enhanced_3)

# Evaluate Enhanced RAG (K_FINAL=3)
results_enhanced_3 = evaluate(
    dataset=dataset_enhanced_3,
    metrics=[faithfulness, answer_relevancy, context_recall, context_precision],
    llm = llm
)

print(results_enhanced_3)

# Convert to DataFrame and take mean of numeric metrics
enhanced_df_3 = results_enhanced_3.to_pandas().mean(numeric_only=True)
print(enhanced_df_3)

# the enhanced RAG evaluation with K_final = 5
data_enhanced_5 = {
    "question": all_questions_sampled,
    "answer": all_enhanced_answers_5,      # Answers from Enhanced RAG with K_FINAL=5
    "contexts": all_enhanced_contexts_5,   # Contexts after Reranking (K_FINAL=5)
    "reference": all_ground_truths_sampled
}

dataset_enhanced_5 = Dataset.from_dict(data_enhanced_5)

# Evaluate Enhanced RAG (K_FINAL=5)
results_enhanced_5 = evaluate(
    dataset=dataset_enhanced_5,
    metrics=[faithfulness, answer_relevancy, context_recall, context_precision],
    llm = llm
)

print(results_enhanced_5)

# Convert to DataFrame and take mean of numeric metrics
enhanced_df_5 = results_enhanced_5.to_pandas().mean(numeric_only=True)
print(enhanced_df_5)